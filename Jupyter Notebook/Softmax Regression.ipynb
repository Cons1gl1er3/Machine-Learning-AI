{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9721de-2dc8-4558-ab2f-7e2cf716860a",
   "metadata": {},
   "source": [
    "When aplly binary classifiers for multi-class, we will face some problems. The one-vs-rest technique has a downside of the sum of probability. Softmax Regresion can effectively solve these shortcoming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450628e-baf7-49b7-b95f-da69d6558cda",
   "metadata": {},
   "source": [
    "The data x has dimension d+1 (add 1 for bias). Consider sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc901de8-d8bc-4e97-939f-592b6d53f06d",
   "metadata": {},
   "source": [
    "$ a_i = sigmoid(z_i) = sigmoid(w_i^{T}\\textbf{x}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5694788-795a-4a67-a2e8-8d37f99bae96",
   "metadata": {},
   "source": [
    "In this technique, $ a_i$, i = 1, 2, ..., C is conducted using $ z_i$ only. However this approach does not create any relation between $ a_i$ and $z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3073e-31c6-4292-bc5f-6b701b9761a8",
   "metadata": {},
   "source": [
    "Note that $ x_i*w_j = z_j$ of $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b2ea2-ed5d-4d02-96b9-fd69d7acc797",
   "metadata": {},
   "source": [
    "So, let's consider Softmax function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f153405-27d4-4897-9127-feb6ab6ef161",
   "metadata": {},
   "source": [
    "$ a_i = \\frac{exp(z_i)}{\\sum_{j=1}^{C}exp(z_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784b5b-310c-4713-8a66-95c42038a8ac",
   "metadata": {},
   "source": [
    "This function satisfies:\n",
    "\n",
    "(i) Compute $a_i $ based on every $ z_i$\n",
    "\n",
    "(ii) $ a_i$ is positive and their sum is equal to 1\n",
    "\n",
    "(iii) $ a_i$ is propotional with $ z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81381e-9b09-466b-9c39-309c885884dc",
   "metadata": {},
   "source": [
    "Let's create softmax function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43e0735-5485-4630-b62c-714d9aabc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    return e_Z/e_Z.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53066a-2ab2-4d27-866b-8e326464078c",
   "metadata": {},
   "source": [
    "In the code above, we subtract every z_i in Z by the maximum z*. This help prevent overflow and reduct the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3ed50-d794-4cb4-8a18-d82410b158f5",
   "metadata": {},
   "source": [
    "To build the loss function, we use Cross Entrophy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92909444-0652-48f5-805a-fddc6d7a17b9",
   "metadata": {},
   "source": [
    "$H(p, q) = -\\sum_{i=1}^{C}p_i\\log(q_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b152576-9ce4-470f-a787-6aa60dff4ff9",
   "metadata": {},
   "source": [
    "Note that $H(p, q)$ differs from $H(q, p)$ and $q_i$ > 0. In this case, we use $p$ to represent the real output, which has only 1 value and 0 value for other, and $q$ to represent predition, which always strictly greater than 0 and smaller than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5fa17-9092-45ec-9d21-532ff15d91c0",
   "metadata": {},
   "source": [
    "Consider the loss function for the softmax regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6eb2ed-c4de-4ade-a8f2-ac05ad035a98",
   "metadata": {},
   "source": [
    "$J(\\textbf{W};\\textbf{X};\\textbf{Y}) = -\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{ij}\\log(a_{ij})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e29ec2-0e19-4150-8982-a53ffe5db1ca",
   "metadata": {},
   "source": [
    "Using Stochastic Gradient Descent with a single point ($x_i, y_i$):\n",
    "\n",
    "$-\\sum_{j=1}^{C}y_{ji}W_j^Tx_i + \\log(\\sum_{k=1}^{C}\\exp(w_k^Tx_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fb381-1873-4bc8-8b46-06393ec8d239",
   "metadata": {},
   "source": [
    "After that, we differentiate the loss function:\n",
    "\n",
    "$\\frac{\\partial J_i(W)}{\\partial W} = [\\frac{\\partial J_i(W)}{\\partial w_1}, \\frac{\\partial J_i(W)}{\\partial w_2}, ..., \\frac{\\partial J_i(W)}{\\partial w_C}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e122f23-bf00-4e9d-9261-c2b14ea9d5a0",
   "metadata": {},
   "source": [
    "Note that gradient of each columns is:\n",
    "\n",
    "$\\frac{\\partial J_i(W)}{\\partial w_j} = e_{ji}x_i$ where $ e_{ji} = a_{ji} - y_{ji} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220151f-1209-4d7f-b72d-f9ac372190ca",
   "metadata": {},
   "source": [
    "In conclusion, we have $\\frac{\\partial J_i(W)}{\\partial W} = x_i[e_{1i}, e_{2i}, ..., e_{Ci}] = x_ie_i^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc454ea-a0ca-4a61-a864-f1f8d1bafa36",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial J(W)}{\\partial W} = \\sum_{i=1}^{N}x_ie_i^T = XE^T$ where $E = A-Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b650550-109b-431b-8c27-dfe077622c73",
   "metadata": {},
   "source": [
    "Update $ W = W+\\eta x_i(y_i-a_i)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fb9c22-c706-4cd4-ad0c-29fbfaee6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 2\n",
    "d = 2\n",
    "C = 3\n",
    "\n",
    "X = np.random.randn(d, N)\n",
    "y = np.random.randint(0, 3, (N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95d8da3-12c5-4b4d-bb05-3da07a8373b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "## One-hot coding  # review this code\n",
    "from scipy import sparse \n",
    "def convert_labels(y, C = C):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), \n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y \n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7c2b275-b7da-4988-bdee-e5b4d59b3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20186097 -1.57735818  2.13312996]\n",
      " [ 0.52527968 -0.57292201 -1.78832804]]\n"
     ]
    }
   ],
   "source": [
    "W_init = np.random.randn(d, C)\n",
    "print(W_init)\n",
    "#print(np.max(W_init, axis = 0, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edb97ad-9373-4cc4-9cde-658f5c213136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02783939 0.14235749]\n",
      " [0.00196556 0.03276953]\n",
      " [0.97019505 0.82487299]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(W_init.T.dot(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec465f-5645-40d9-8fe6-f162c79fa639",
   "metadata": {},
   "source": [
    "Now we need to revalidate the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ced07a0-78c5-4340-b920-b4a7fece4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.999559724142033\n"
     ]
    }
   ],
   "source": [
    "def cost(X, Y, W):\n",
    "    A = softmax(W.T.dot(X))\n",
    "    return -np.sum(Y*np.log(A))\n",
    "print(cost(X, Y, W_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad8e2dde-3924-4524-ab32-5e523fb8e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.035166276268107e-10\n"
     ]
    }
   ],
   "source": [
    "def grad(X, Y, W):\n",
    "    A = softmax(W.T.dot(X))\n",
    "    E = A - Y\n",
    "    return X.dot(E.T)\n",
    "\n",
    "def numerical_grad(X, Y, W, cost):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_p = W.copy()\n",
    "            W_n = W.copy()\n",
    "            W_p[i, j] += eps \n",
    "            W_n[i, j] -= eps\n",
    "            g[i,j] = (cost(X, Y, W_p) - cost(X, Y, W_n))/(2*eps)\n",
    "    return g\n",
    "\n",
    "g1 = grad(X, Y, W_init)\n",
    "g2 = numerical_grad(X, Y, W_init, cost)\n",
    "print(np.linalg.norm(g1-g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f7197cd-5b33-4ca3-a62b-108f97cb0697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.35457126  0.96018019 -1.5571187 ]\n",
      " [-0.00383276 -0.37226664 -1.45987098]]\n"
     ]
    }
   ],
   "source": [
    "# Main function for Softmax Regression\n",
    "def SoftmaxRegression(X, y, W_init, eta, max_iter = 10000):\n",
    "    W = W_init\n",
    "    C = W_init.shape[1]\n",
    "    d = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    Y = convert_labels(y, C)\n",
    "    iter = 0 # Count the number of iters\n",
    "    check_w_after = 20\n",
    "    while iter < max_iter:\n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = Y[:, i].reshape(C, 1)\n",
    "            ai = softmax(W.T.dot(xi))\n",
    "            W_new = W + eta*xi.dot((yi-ai).T)\n",
    "            iter += 1\n",
    "            if iter%check_w_after == 0:\n",
    "                if np.linalg.norm(W_new - W)<1e-4:\n",
    "                    return W_new\n",
    "                W = W_new\n",
    "    return W\n",
    "eta = 0.05\n",
    "\n",
    "W = SoftmaxRegression(X, y, W_init, eta)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0d01b63-c8a8-4320-9c58-c87121ea7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def pred(W, X):\n",
    "    A = softmax_stable(W.T.dot(X))\n",
    "    return np.argmax(A, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bf4dc-50a5-4b4a-9fda-a68f30a222ee",
   "metadata": {},
   "source": [
    "Note: boundary created by softmax regression is linear. This is because a point x is predicted to be a member of class k if $a_j \\leq a_k, \\forall j \\neq k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
